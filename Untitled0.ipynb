{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNP6uSHFY16Qgj58GDR7tFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinChangoluisa/FUNDAMENTOS-DE-NODE.JS/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37lT_kU6H4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65dbc2aa-467b-40cb-aadb-2a844f4099f4"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun May  3 17:50:26 2020\n",
        "\n",
        "@author: \n",
        "CHANGOLUISA KEVIN\n",
        "GUEVARA ANDRES\n",
        "CENTENO KATHERINE\n",
        "\"\"\"\n",
        "#Lectura de pagina web y extraccion de titulos\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "file = urlopen(\"https://www.journals.elsevier.com/journal-of-english-for-academic-purposes/recent-articles\")\n",
        "html = file.read()\n",
        "file.close()\n",
        "\n",
        "soup=BeautifulSoup(html, \"lxml\")\n",
        "busca=soup.find_all(\"a\")\n",
        "titulos=[]\n",
        "\n",
        "for links in busca:\n",
        "    titulos.append(links.get('title'))\n",
        "    \n",
        "titulos = list(filter(None, titulos)) \n",
        "\n",
        "titulos_ok=titulos[34:43]\n",
        "\n",
        "doc=[]\n",
        "\n",
        "for i in range(len(titulos_ok)):\n",
        "    doc.append([titulos_ok[i]])\n",
        "    \n",
        "\n",
        "#curacion de documentos\n",
        "import re\n",
        "\n",
        "def data_curation(data):\n",
        "    data_ok=data.lower()\n",
        "    data_ok=re.sub('[^a-z0-9]+', ' ', data_ok)\n",
        "    data_ok=data_ok.split()\n",
        "    return data_ok\n",
        "\n",
        "    \n",
        "t1=data_curation(str(doc[0]))\n",
        "t2=data_curation(str(doc[1]))\n",
        "t3=data_curation(str(doc[2]))\n",
        "t4=data_curation(str(doc[3]))\n",
        "t5=data_curation(str(doc[4]))\n",
        "t6=data_curation(str(doc[5]))\n",
        "t7=data_curation(str(doc[6]))\n",
        "t8=data_curation(str(doc[7]))\n",
        "t9=data_curation(str(doc[8]))\n",
        "\n",
        "\n",
        "#eliminacion de stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "a_stop = stopwords.words('english')\n",
        "\n",
        "def stopw(data):\n",
        "    for word in data:\n",
        "        if(word in a_stop):\n",
        "            data.remove(word)        \n",
        "    return data\n",
        "\n",
        "t1=stopw(t1)\n",
        "t2=stopw(t2)\n",
        "t3=stopw(t3)\n",
        "t4=stopw(t4)\n",
        "t5=stopw(t5)\n",
        "t6=stopw(t6)\n",
        "t7=stopw(t7)\n",
        "t8=stopw(t8)\n",
        "t9=stopw(t9)\n",
        "\n",
        "\n",
        "#stemmer \n",
        "\n",
        "stemmer=PorterStemmer() \n",
        "\n",
        "def steem(data):\n",
        "    data1=[]\n",
        "    for i in data:\n",
        "        data1.append(stemmer.stem(i))\n",
        "    return data1\n",
        "\n",
        "t1=steem(t1)\n",
        "t2=steem(t2)\n",
        "t3=steem(t3)\n",
        "t4=steem(t4)\n",
        "t5=steem(t5)\n",
        "t6=steem(t6)\n",
        "t7=steem(t7)\n",
        "t8=steem(t8)\n",
        "t9=steem(t9)\n",
        "\n",
        "'''Se crea una funcion para agregar palabras que no se repiten al \n",
        "diccionario, para buscar palabras usamos in en el if '''\n",
        "dicc=[]\n",
        "def diccionario(data):\n",
        "    for i in range(len(data)):\n",
        "        if data[i] in dicc:\n",
        "            False\n",
        "        else:\n",
        "            dicc.append(data[i])\n",
        "\n",
        "diccionario(t1)\n",
        "diccionario(t2)\n",
        "diccionario(t3)\n",
        "diccionario(t4)\n",
        "diccionario(t5)\n",
        "diccionario(t6)\n",
        "diccionario(t7)\n",
        "diccionario(t8)\n",
        "diccionario(t9)\n",
        "dicc=sorted(dicc)\n",
        "\n",
        "'''Creamos una blblioteca con nuestros documentos curados'''\n",
        "biblioteca=[t1,t2,t3,t4,t5,t6,t7,t8,t9]\n",
        "\n",
        "'''Creamos una funcion para definir la posicion de cada palabra en el\n",
        "documento'''\n",
        "\n",
        "pos=[]\n",
        "\n",
        "def posicion(palabra,documento):   \n",
        "    for i in range(len(documento)):\n",
        "        if palabra in documento[i]:\n",
        "            pos.append(i+1)\n",
        "    return pos\n",
        "    \n",
        "'''Creamos una funcion para obtener el numero de veces que se repite\n",
        "una palabra'''\n",
        "\n",
        "rep=[]\n",
        "def repeticiones(num_docu,palabra,documento):\n",
        "    n_veces=documento.count(palabra)\n",
        "    rep.append(num_docu)\n",
        "    rep.append(n_veces)    \n",
        "    return rep\n",
        "\n",
        "concurrencias=[]\n",
        "final=[]\n",
        "\n",
        "for j in range (len(dicc)):\n",
        "    concurrencias.append(dicc[j])\n",
        "    for i in range(len(biblioteca)):\n",
        "        posicion(dicc[j],biblioteca[i])\n",
        "        repeticiones(i+1,dicc[j],biblioteca[i])\n",
        "        if rep[1]!=0:\n",
        "            rep.append(pos) \n",
        "            concurrencias.append(rep)  \n",
        "        rep=[]\n",
        "        pos=[] \n",
        "    print(concurrencias)\n",
        "    final.append(concurrencias)\n",
        "    concurrencias=[]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['academ', [2, 1, [8]], [8, 1, [8]]]\n",
            "['across', [4, 1, [3]]]\n",
            "['author', [1, 1, [2]]]\n",
            "['base', [7, 1, [3]]]\n",
            "['bundl', [5, 1, [5]]]\n",
            "['china', [6, 1, [9]]]\n",
            "['chines', [2, 1, [11]], [9, 1, [3]]]\n",
            "['collabor', [2, 1, [7]]]\n",
            "['colleg', [2, 1, [14]]]\n",
            "['commun', [8, 1, [9]]]\n",
            "['compet', [1, 1, [8]]]\n",
            "['complex', [4, 1, [2]]]\n",
            "['construct', [1, 1, [6]]]\n",
            "['develop', [2, 1, [1]]]\n",
            "['discuss', [3, 1, [4]]]\n",
            "['dissert', [3, 1, [3]], [7, 1, [4]]]\n",
            "['doctor', [3, 1, [2]]]\n",
            "['educ', [7, 1, [2]]]\n",
            "['effect', [2, 1, [2]]]\n",
            "['engag', [3, 1, [1]]]\n",
            "['english', [3, 1, [7]], [5, 1, [2]], [6, 1, [4]], [9, 1, [5]]]\n",
            "['essay', [5, 1, [6]]]\n",
            "['evid', [2, 1, [10]]]\n",
            "['examin', [7, 1, [1]]]\n",
            "['explor', [8, 1, [4]]]\n",
            "['first', [2, 1, [12]]]\n",
            "['formal', [4, 1, [6]]]\n",
            "['grammat', [8, 1, [6]]]\n",
            "['ident', [1, 1, [5]]]\n",
            "['instruct', [6, 1, [6]]]\n",
            "['investig', [4, 1, [5]]]\n",
            "['japan', [6, 1, [8]]]\n",
            "['korean', [5, 1, [8]]]\n",
            "['languag', [4, 1, [8]]]\n",
            "['lexic', [5, 1, [4]]]\n",
            "['macrostructur', [7, 1, [5]]]\n",
            "['medium', [6, 1, [5]]]\n",
            "['metaphor', [8, 1, [7]]]\n",
            "['method', [2, 1, [6]]]\n",
            "['nativ', [3, 1, [8]]]\n",
            "['power', [8, 1, [5]]]\n",
            "['pragmat', [1, 1, [7]]]\n",
            "['preposit', [5, 1, [3]]]\n",
            "['programm', [6, 1, [7]]]\n",
            "['provis', [6, 1, [1]]]\n",
            "['read', [2, 1, [9]]]\n",
            "['refer', [1, 1, [4]]]\n",
            "['regist', [4, 1, [4]]]\n",
            "['second', [4, 1, [7]]]\n",
            "['section', [3, 1, [5]]]\n",
            "['self', [1, 1, [3]]]\n",
            "['singl', [1, 1, [1]]]\n",
            "['speaker', [3, 1, [9]]]\n",
            "['stage', [2, 1, [4]]]\n",
            "['student', [2, 1, [15]], [5, 1, [10]], [6, 1, [2]], [9, 1, [4]]]\n",
            "['support', [6, 1, [3]]]\n",
            "['syntact', [4, 1, [1]]]\n",
            "['teach', [2, 1, [5]]]\n",
            "['three', [2, 1, [3]]]\n",
            "['trudeau', [8, 1, [3]]]\n",
            "['trump', [8, 1, [1]]]\n",
            "['understand', [9, 1, [1]]]\n",
            "['univers', [5, 1, [9]]]\n",
            "['use', [5, 1, [1]]]\n",
            "['voic', [9, 1, [2]]]\n",
            "['vs', [8, 1, [2]]]\n",
            "['write', [4, 1, [9]], [9, 1, [6]]]\n",
            "['written', [3, 1, [6]], [5, 1, [7]]]\n",
            "['year', [2, 1, [13]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}